from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain.schema import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import FAISS
from pathlib import Path
import pdfplumber
import tiktoken
import re
from tiktoken import encoding_for_model
import os

# PDF ìƒì„±ì„ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
from reportlab.lib.pagesizes import A4
from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, PageBreak
from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle
from reportlab.lib.units import inch
from reportlab.pdfbase import pdfmetrics
from reportlab.pdfbase.ttfonts import TTFont
from reportlab.lib.colors import HexColor
from datetime import datetime


def setup_korean_font():
    """í•œê¸€ í°íŠ¸ ì„¤ì • - ì‹œìŠ¤í…œì— ìˆëŠ” í•œê¸€ í°íŠ¸ë¥¼ ì°¾ì•„ì„œ ë“±ë¡"""
    try:
        # Windowsì˜ ê²½ìš°
        font_paths = [
            "C:/Windows/Fonts/malgun.ttf",  # ë§‘ì€ ê³ ë”•
            "C:/Windows/Fonts/gulim.ttc",   # êµ´ë¦¼
            "C:/Windows/Fonts/batang.ttc",  # ë°”íƒ•
        ]
        
        for font_path in font_paths:
            if os.path.exists(font_path):
                pdfmetrics.registerFont(TTFont('Korean', font_path))
                return 'Korean'
        
        # macOSì˜ ê²½ìš°
        mac_font_paths = [
            "/System/Library/Fonts/AppleSDGothicNeo.ttc",
            "/Library/Fonts/AppleGothic.ttf",
        ]
        
        for font_path in mac_font_paths:
            if os.path.exists(font_path):
                pdfmetrics.registerFont(TTFont('Korean', font_path))
                return 'Korean'
                
        print("âš ï¸  í•œê¸€ í°íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ í°íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        return 'Helvetica'
        
    except Exception as e:
        print(f"âš ï¸  í°íŠ¸ ì„¤ì • ì¤‘ ì˜¤ë¥˜: {e}. ê¸°ë³¸ í°íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        return 'Helvetica'

def create_pdf_summary(summary_text, output_path, university_name):
    """ìš”ì•½ í…ìŠ¤íŠ¸ë¥¼ PDFë¡œ ì €ì¥"""
    try:
        # í•œê¸€ í°íŠ¸ ì„¤ì •
        korean_font = setup_korean_font()
        
        # PDF ë¬¸ì„œ ìƒì„±
        doc = SimpleDocTemplate(
            str(output_path),
            pagesize=A4,
            rightMargin=inch * 0.7,
            leftMargin=inch * 0.7,
            topMargin=inch * 0.8,
            bottomMargin=inch * 0.8
        )
        
        # ìŠ¤íƒ€ì¼ ì •ì˜
        styles = getSampleStyleSheet()
        
        # ì œëª© ìŠ¤íƒ€ì¼
        title_style = ParagraphStyle(
            'CustomTitle',
            parent=styles['Title'],
            fontName=korean_font,
            fontSize=18,
            spaceAfter=30,
            textColor=HexColor('#2c3e50'),
            alignment=1  # ì¤‘ì•™ ì •ë ¬
        )
        
        # ë¶€ì œëª© ìŠ¤íƒ€ì¼ (## ì„¹ì…˜)
        heading_style = ParagraphStyle(
            'CustomHeading',
            parent=styles['Heading1'],
            fontName=korean_font,
            fontSize=14,
            spaceBefore=20,
            spaceAfter=10,
            textColor=HexColor('#34495e'),
            leftIndent=0
        )
        
        # ì†Œì œëª© ìŠ¤íƒ€ì¼ (### ì„¹ì…˜)
        subheading_style = ParagraphStyle(
            'CustomSubHeading',
            parent=styles['Heading2'],
            fontName=korean_font,
            fontSize=12,
            spaceBefore=15,
            spaceAfter=8,
            textColor=HexColor('#7f8c8d'),
            leftIndent=10
        )
        
        # ë³¸ë¬¸ ìŠ¤íƒ€ì¼
        body_style = ParagraphStyle(
            'CustomBody',
            parent=styles['Normal'],
            fontName=korean_font,
            fontSize=10,
            spaceAfter=6,
            leftIndent=0,
            rightIndent=0,
            leading=14
        )
        
        # ë¦¬ìŠ¤íŠ¸ ìŠ¤íƒ€ì¼
        list_style = ParagraphStyle(
            'CustomList',
            parent=styles['Normal'],
            fontName=korean_font,
            fontSize=10,
            spaceAfter=4,
            leftIndent=20,
            bulletIndent=10,
            leading=13
        )
        
        # PDF ë‚´ìš© êµ¬ì„±
        story = []
        
        # ì œëª© ì¶”ê°€
        story.append(Paragraph(f"ğŸ« {university_name} ì…ì‹œìš”ê°• ìš”ì•½", title_style))
        story.append(Spacer(1, 12))
        
        # ìƒì„± ì¼ì‹œ ì¶”ê°€
        current_time = datetime.now().strftime("%Yë…„ %mì›” %dì¼ %H:%M")
        story.append(Paragraph(f"ìƒì„±ì¼ì‹œ: {current_time}", body_style))
        story.append(Spacer(1, 20))
        
        # ìš”ì•½ ë‚´ìš© íŒŒì‹± ë° ì¶”ê°€
        lines = summary_text.split('\n')
        
        for line in lines:
            line = line.strip()
            if not line:
                story.append(Spacer(1, 6))
                continue
            
            # ë§ˆí¬ë‹¤ìš´ í˜•ì‹ ì²˜ë¦¬
            if line.startswith('## '):
                # ì£¼ìš” ì„¹ì…˜ ì œëª©
                title = line[3:].strip()
                story.append(Paragraph(title, heading_style))
                
            elif line.startswith('### '):
                # í•˜ìœ„ ì„¹ì…˜ ì œëª©
                title = line[4:].strip()
                story.append(Paragraph(title, subheading_style))
                
            elif line.startswith('* **') and '**:' in line:
                # êµµì€ ê¸€ì”¨ê°€ ìˆëŠ” ë¦¬ìŠ¤íŠ¸ í•­ëª©
                content = line[2:].strip()
                # **í…ìŠ¤íŠ¸**: ë¶€ë¶„ì„ ì²˜ë¦¬
                if '**:' in content:
                    bold_part, rest = content.split('**:', 1)
                    bold_part = bold_part.replace('**', '')
                    formatted_content = f"<b>{bold_part}</b>: {rest.strip()}"
                else:
                    formatted_content = content
                story.append(Paragraph(f"â€¢ {formatted_content}", list_style))
                
            elif line.startswith('* ') or line.startswith('- '):
                # ì¼ë°˜ ë¦¬ìŠ¤íŠ¸ í•­ëª©
                content = line[2:].strip()
                story.append(Paragraph(f"â€¢ {content}", list_style))
                
            elif line.startswith(('1. ', '2. ', '3. ', '4. ', '5. ', '6. ', '7. ', '8. ', '9. ')):
                # ë²ˆí˜¸ ë¦¬ìŠ¤íŠ¸ í•­ëª©
                story.append(Paragraph(line, list_style))
                
            elif line.startswith('---'):
                # êµ¬ë¶„ì„ 
                story.append(Spacer(1, 15))
                
            elif line and not line.startswith('#'):
                # ì¼ë°˜ ë³¸ë¬¸
                # êµµì€ ê¸€ì”¨ ì²˜ë¦¬
                formatted_line = line.replace('**', '<b>', 1).replace('**', '</b>', 1) if '**' in line else line
                story.append(Paragraph(formatted_line, body_style))
        
        # PDF ìƒì„±
        doc.build(story)
        return True
        
    except Exception as e:
        print(f"âŒ PDF ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
        return False

def count_tokens(text):
    encoder = encoding_for_model("gpt-4o")
    return len(encoder.encode(text))

def extract_text_and_tables(file_path):
    text = ""
    with pdfplumber.open(file_path) as pdf:
        for page in pdf.pages:
            # í…ìŠ¤íŠ¸ ì¶”ì¶œ (ë ˆì´ì•„ì›ƒ ìœ ì§€)
            page_text = page.extract_text(layout=True) or ""
            lines = page_text.splitlines()
            
            # ì…ì‹œìš”ê°• ê´€ë ¨ í‚¤ì›Œë“œ íŒ¨í„´ì„ í•µì‹¬ í•­ëª©ì— ë§ì¶° í™•ì¥
            patterns = [
                # ì…í•™ìê²© ê´€ë ¨
                r"ì…í•™ìê²©|ì§€ì›ìê²©|í•™ë ¥|ì¡¸ì—…|ì¬ì§|ê²½ë ¥",
                # ëª¨ì§‘ì •ì› ê´€ë ¨  
                r"ì •ì›|ì¸ì›|ëª¨ì§‘|ì„ ë°œ",
                # ì…ì‹œì¼ì • ê´€ë ¨
                r"ì¼ì •|ì¼ì •í‘œ|ì‹œê°„í‘œ|ì ‘ìˆ˜|ì‹ ì²­|ì§€ì›|í•©ê²©|ë°œí‘œ|ë©´ì ‘|ê³ ì‚¬|ì‹œí—˜",
                # ì „í˜•ë°©ë²• ê´€ë ¨
                r"ì „í˜•|ë°©ë²•|ê¸°ì¤€|í‰ê°€|ìˆ˜ì‹œ|ì •ì‹œ|ì¢…í•©|êµê³¼",
                # ìˆ˜ëŠ¥ìµœì €í•™ë ¥ê¸°ì¤€ ê´€ë ¨
                r"ìµœì €|í•™ë ¥|ê¸°ì¤€|ë“±ê¸‰|ìˆ˜ëŠ¥|ì ìˆ˜|ì„±ì ",
                # ì œì¶œì„œë¥˜ ê´€ë ¨
                r"ì„œë¥˜|ì œì¶œ|êµ¬ë¹„|ì¦ëª…ì„œ|ì¶”ì²œì„œ|ìê¸°ì†Œê°œì„œ",
                # ë“±ë¡ê¸ˆ ê´€ë ¨
                r"ë“±ë¡ê¸ˆ|í•™ë¹„|ìˆ˜ì—…ë£Œ|ë‚©ì…|ì›|ì²œì›|ë§Œì›",
                # ì¥í•™ê¸ˆ ê´€ë ¨
                r"ì¥í•™ê¸ˆ|ì§€ì›ê¸ˆ|ê°ë©´|í˜œíƒ|ì„±ì ìš°ìˆ˜|ì €ì†Œë“",
                # ìœ ì˜ì‚¬í•­ ê´€ë ¨
                r"ìœ ì˜|ì£¼ì˜|ì¤‘ë³µ|ì œí•œ|ê¸ˆì§€|ë§ˆê°|í™˜ë¶ˆ",
                # ê¸°íƒ€ ì¤‘ìš”ì‚¬í•­ ê´€ë ¨
                r"ê¸°ìˆ™ì‚¬|êµí™˜|í¸ì…|ë³µìˆ˜|íŠ¹ë³„|í•´ì™¸|ì—°ìˆ˜"
            ]
            
            # ê´€ë ¨ ë¼ì¸ í•„í„°ë§ (ë” í¬ê´„ì ìœ¼ë¡œ)
            relevant_lines = []
            for line in lines:
                line_stripped = line.strip()
                if not line_stripped or "í˜ì´ì§€" in line or "ê´‘ê³ " in line:
                    continue
                    
                # í‚¤ì›Œë“œ ë§¤ì¹­ ë˜ëŠ” ë‚ ì§œ/ìˆ«ì íŒ¨í„´ ë§¤ì¹­
                if (any(re.search(p, line, re.IGNORECASE) for p in patterns) or
                    any(str(y) in line for y in range(2020, 2030)) or
                    any(month in line for month in ["1ì›”", "2ì›”", "3ì›”", "4ì›”", "5ì›”", "6ì›”", "7ì›”", "8ì›”", "9ì›”", "10ì›”", "11ì›”", "12ì›”"]) or
                    any(day in line for day in [f"{i}ì¼" for i in range(1, 32)]) or
                    re.search(r'\d+ì›|\d+ëª…|\d+ì ', line) or  # ê¸ˆì•¡, ì¸ì›, ì ìˆ˜
                    re.search(r'\d{4}\.\d{1,2}\.\d{1,2}', line)):  # ë‚ ì§œ í˜•ì‹
                    relevant_lines.append(line)
            
            text += "\n".join(relevant_lines) + "\n"
            
            # í‘œ ì¶”ì¶œ
            tables = page.extract_tables()
            for table in tables:
                if table:  # ë¹ˆ í…Œì´ë¸” ì œì™¸
                    for row in table:
                        if row and any(cell for cell in row if cell):  # ë¹ˆ í–‰ ì œì™¸
                            text += " | ".join([str(cell) if cell else "" for cell in row]) + "\n"
    
    return [Document(page_content=text)]

def process_documents(file_path):
    # ë‹¨ê³„ 1: ë¬¸ì„œ ë¡œë“œ
    docs = extract_text_and_tables(file_path)
    print(f"ì¶”ì¶œëœ ë¬¸ì„œ ìˆ˜: {len(docs)}")
    
    # ì¶”ì¶œëœ í…ìŠ¤íŠ¸ ì €ì¥ (ë””ë²„ê¹…ìš©)
    debug_file = Path(file_path).parent / f"{Path(file_path).stem}_extracted.txt"
    with open(debug_file, "w", encoding="utf-8") as f:
        f.write(docs[0].page_content)
    
    print(f"ì¶”ì¶œëœ í…ìŠ¤íŠ¸ ë¯¸ë¦¬ë³´ê¸°:\n{docs[0].page_content[:1500]}...")
    print(f"ì¶”ì¶œëœ í…ìŠ¤íŠ¸ í† í° ìˆ˜: {count_tokens(docs[0].page_content)}")

    # ë‹¨ê³„ 2: ë¬¸ì„œ ë¶„í•  (ì²­í¬ í¬ê¸° ì¡°ì •)
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=2500,  # ì²­í¬ í¬ê¸° ì¤„ì„
        chunk_overlap=300,
        separators=["\n\n", "\n", ".", "!", "?", ",", " ", ""]
    )
    split_documents = text_splitter.split_documents(docs)
    print(f"ë¶„í• ëœ ë¬¸ì„œ ì¡°ê° ìˆ˜: {len(split_documents)}")

    # ë‹¨ê³„ 3: ì„ë² ë”© ìƒì„±
    embeddings = OpenAIEmbeddings()

    # ë‹¨ê³„ 4: ë²¡í„°ìŠ¤í† ì–´ ìƒì„±
    vectorstore = FAISS.from_documents(documents=split_documents, embedding=embeddings)

    # ë‹¨ê³„ 5: ê²€ìƒ‰ê¸° ìƒì„± (ë” ë§ì€ ì¡°ê° ê²€ìƒ‰)
    retriever = vectorstore.as_retriever(
        search_type="mmr", 
        search_kwargs={"k": 20, "fetch_k": 30}  # ë” ë§ì€ ì¡°ê° ê²€ìƒ‰
    )

    # ë‹¨ê³„ 6: í•µì‹¬ í•­ëª© ì¤‘ì‹¬ í”„ë¡¬í”„íŠ¸ ìƒì„±
    prompt = ChatPromptTemplate.from_messages([
        ("system", """ë‹¹ì‹ ì€ ëŒ€í•™ ì…ì‹œ ì „ë¬¸ ìƒë‹´ì‚¬ì…ë‹ˆë‹¤. 
ì…ì‹œìš”ê°• ë¬¸ì„œë¥¼ ì •í™•í•˜ê³  ì²´ê³„ì ìœ¼ë¡œ ë¶„ì„í•˜ì—¬ ìˆ˜í—˜ìƒê³¼ í•™ë¶€ëª¨ê°€ í•„ìš”ë¡œ í•˜ëŠ” í•µì‹¬ ì •ë³´ë¥¼ ëª…í™•í•˜ê²Œ ì •ë¦¬í•´ì£¼ì„¸ìš”.
ì •ë³´ê°€ ì—†ëŠ” ê²½ìš° "ì •ë³´ ì—†ìŒ" ë˜ëŠ” "ë¬¸ì„œì—ì„œ í™•ì¸ë˜ì§€ ì•ŠìŒ"ìœ¼ë¡œ í‘œì‹œí•˜ì„¸ìš”."""),
        
        ("human", """
ë‹¤ìŒì€ ëŒ€í•™ ì…ì‹œìš”ê°• PDFì—ì„œ ì¶”ì¶œí•œ í…ìŠ¤íŠ¸ì…ë‹ˆë‹¤. 
ì•„ë˜ 10ê°œ í•µì‹¬ í•­ëª©ì— ë§ì¶° ì •í™•í•˜ê²Œ ìš”ì•½í•´ì£¼ì„¸ìš”:

## ğŸ“ ì…í•™ ìê²©
ì§€ì› ê°€ëŠ¥í•œ í•™ë ¥ ì¡°ê±´ê³¼ ìê²© ìš”ê±´ì„ ì •ë¦¬í•´ì£¼ì„¸ìš”.
í˜•ì‹ ì˜ˆì‹œ:
* **ê¸°ë³¸ ìê²©**: ê³ ë“±í•™êµ ì¡¸ì—… ì´ìƒ ë˜ëŠ” ë™ë“± í•™ë ¥ ì¸ì •ì
* **ì¬ì§ì íŠ¹ë³„ì „í˜•**: ì¬ì§ ê²½ë ¥ 3ë…„ ì´ìƒ
* **ì™¸êµ­ì¸ íŠ¹ë³„ì „í˜•**: ì™¸êµ­ êµ­ì ì ë˜ëŠ” í•´ì™¸ ê³ êµ ì¡¸ì—…ì

---

## ğŸ‘¥ ëª¨ì§‘ ì •ì›
í•™ê³¼ë³„ ë˜ëŠ” ì „í˜•ë³„ ëª¨ì§‘ì¸ì›ì„ ì •ë¦¬í•´ì£¼ì„¸ìš”.
í˜•ì‹ ì˜ˆì‹œ:
* **ì»´í“¨í„°ê³µí•™ê³¼**: 50ëª…
* **ê²½ì˜í•™ê³¼**: ì •ì› ë¯¸ìƒ
* **ì˜ì˜ˆê³¼**: 40ëª… (ì •ì‹œ 20ëª…, ìˆ˜ì‹œ 20ëª…)

---

## ğŸ“‹ ì…ì‹œ ì¼ì •
ì£¼ìš” ì¼ì •ì„ ì‹œê°„ìˆœìœ¼ë¡œ ì •ë¦¬í•´ì£¼ì„¸ìš”.
í˜•ì‹ ì˜ˆì‹œ:
1. **ì›ì„œì ‘ìˆ˜**:
   * ê¸°ê°„: 2024ë…„ 9ì›” 9ì¼(ì›”) 09:00 ~ 9ì›” 13ì¼(ê¸ˆ) 18:00
2. **ì„œë¥˜ì œì¶œ**:
   * ê¸°ê°„: 2024ë…„ 9ì›” 9ì¼(ì›”) 09:00 ~ 9ì›” 20ì¼(ê¸ˆ) 18:00
3. **1ë‹¨ê³„ í•©ê²©ì ë°œí‘œ**:
   * ì‹¤ê¸°/ì‹¤ì  ì „í˜•: 2024ë…„ 10ì›” 10ì¼(ëª©) 16:00
4. **ë©´ì ‘ ë° ê³ ì‚¬ì¼**:
   * í•™ìƒë¶€ì¢…í•©ì „í˜•: 2024ë…„ 11ì›” 16ì¼(í† )
5. **ìµœì¢… í•©ê²©ì ë°œí‘œ**:
   * 2024ë…„ 12ì›” 20ì¼(ê¸ˆ) 15:00

---

## ğŸ“š ì „í˜• ë°©ë²•
ì…ì‹œ ì „í˜• ìœ í˜•ê³¼ í‰ê°€ ê¸°ì¤€ì„ ì •ë¦¬í•´ì£¼ì„¸ìš”.
í˜•ì‹ ì˜ˆì‹œ:
### ìˆ˜ì‹œëª¨ì§‘
* **í•™ìƒë¶€ì¢…í•©ì „í˜•**: ì„œë¥˜í‰ê°€ 70% + ë©´ì ‘ 30%
* **í•™ìƒë¶€êµê³¼ì „í˜•**: í•™ìƒë¶€ 100%

### ì •ì‹œëª¨ì§‘
* **ê°€êµ°**: ìˆ˜ëŠ¥ 100%
* **ë‚˜êµ°**: ìˆ˜ëŠ¥ 80% + í•™ìƒë¶€ 20%

---

## ğŸ“Š ëŒ€í•™ìˆ˜í•™ëŠ¥ë ¥ì‹œí—˜ ìµœì €í•™ë ¥ê¸°ì¤€
ìˆ˜ëŠ¥ ìµœì €í•™ë ¥ê¸°ì¤€ì„ ì •ë¦¬í•´ì£¼ì„¸ìš”.
í˜•ì‹ ì˜ˆì‹œ:
* **ì¸ë¬¸ê³„ì—´**: êµ­ì–´, ìˆ˜í•™, ì˜ì–´, íƒêµ¬ ì¤‘ 3ê°œ ì˜ì—­ ë“±ê¸‰ í•© 7 ì´ë‚´
* **ìì—°ê³„ì—´**: êµ­ì–´, ìˆ˜í•™, ì˜ì–´, ê³¼íƒ ì¤‘ 3ê°œ ì˜ì—­ ë“±ê¸‰ í•© 6 ì´ë‚´
* **ì˜ì˜ˆê³¼**: êµ­ì–´, ìˆ˜í•™, ì˜ì–´, ê³¼íƒ ëª¨ë‘ 1ë“±ê¸‰

---

## ğŸ“„ ì œì¶œ ì„œë¥˜
ì§€ì› ì‹œ í•„ìš”í•œ ì„œë¥˜ ëª©ë¡ì„ ì •ë¦¬í•´ì£¼ì„¸ìš”.
í˜•ì‹ ì˜ˆì‹œ:
* **ê³µí†µ ì„œë¥˜**: ì¡¸ì—…ì¦ëª…ì„œ, í•™êµìƒí™œê¸°ë¡ë¶€
* **í•™ìƒë¶€ì¢…í•©ì „í˜•**: ìê¸°ì†Œê°œì„œ, ì¶”ì²œì„œ 1ë¶€
* **íŠ¹ë³„ì „í˜•**: ì¬ì§ì¦ëª…ì„œ, ê²½ë ¥ì¦ëª…ì„œ
* **ì™¸êµ­ì¸ ì „í˜•**: ì™¸êµ­ì¸ë“±ë¡ì¦, í•œêµ­ì–´ëŠ¥ë ¥ì‹œí—˜ ì„±ì í‘œ

---

## ğŸ’° ë“±ë¡ê¸ˆ ì •ë³´
í•™ê³¼ë³„ ë˜ëŠ” ê³„ì—´ë³„ ë“±ë¡ê¸ˆì„ ì •ë¦¬í•´ì£¼ì„¸ìš”.
í˜•ì‹ ì˜ˆì‹œ:
* **ì˜ê³¼ëŒ€í•™(ì˜ì˜ˆê³¼)**: 3,156,500ì›
* **ì¹˜ê³¼ëŒ€í•™(ì¹˜ì˜ì˜ˆê³¼)**: 3,156,500ì›
* **ê³µê³¼ëŒ€í•™/ITëŒ€í•™**: 2,331,000ì›
* **ì¸ë¬¸ëŒ€í•™**: 1,781,000ì›

---

## ğŸ“ ì¥í•™ê¸ˆ ì •ë³´
ì¥í•™ê¸ˆ ì¢…ë¥˜ì™€ ì§€ì› ì¡°ê±´ì„ ì •ë¦¬í•´ì£¼ì„¸ìš”.
í˜•ì‹ ì˜ˆì‹œ:
* **ì„±ì ìš°ìˆ˜ì¥í•™ê¸ˆ**: ì…í•™ì„±ì  ìƒìœ„ 10% ì´ë‚´, ë“±ë¡ê¸ˆ ì „ì•¡ ì§€ì›
* **ì§€ì—­ì¸ì¬ì¥í•™ê¸ˆ**: íŠ¹ì • ì§€ì—­ ì¶œì‹ , ë“±ë¡ê¸ˆ 50% ê°ë©´
* **ì €ì†Œë“ì¸µì§€ì›ì¥í•™ê¸ˆ**: ê¸°ì´ˆìƒí™œìˆ˜ê¸‰ì, ë“±ë¡ê¸ˆ ì „ì•¡ + ìƒí™œë¹„ ì§€ì›

---

## âš ï¸ ìœ ì˜ì‚¬í•­
ì§€ì› ì‹œ ì£¼ì˜í•  ì‚¬í•­ì„ ì •ë¦¬í•´ì£¼ì„¸ìš”.
í˜•ì‹ ì˜ˆì‹œ:
* **ì¤‘ë³µì§€ì› ì œí•œ**: ìˆ˜ì‹œëª¨ì§‘ 6íšŒ ì´ë‚´, ì •ì‹œëª¨ì§‘ 3íšŒ ì´ë‚´
* **ì„œë¥˜ ì œì¶œ**: ë§ˆê°ì¼ 18:00ê¹Œì§€ ë„ì°©ë¶„ì— í•œí•¨
* **ë©´ì ‘ ë¶ˆì°¸**: ë©´ì ‘ ë¶ˆì°¸ ì‹œ ë¶ˆí•©ê²© ì²˜ë¦¬
* **ë“±ë¡ í¬ê¸°**: ë“±ë¡ í¬ê¸° ì‹œ í™˜ë¶ˆ ê·œì • ì ìš©

---

## ğŸ“Œ ê¸°íƒ€ ì¤‘ìš”ì‚¬í•­
ê¸°ìˆ™ì‚¬, êµí™˜í•™ìƒ, íŠ¹ë³„ì „í˜• ë“± ê¸°íƒ€ ì •ë³´ë¥¼ ì •ë¦¬í•´ì£¼ì„¸ìš”.
í˜•ì‹ ì˜ˆì‹œ:
* **ê¸°ìˆ™ì‚¬**: ì‹ ì…ìƒ ìš°ì„  ë°°ì •, ì›” 30ë§Œì›
* **êµí™˜í•™ìƒ**: 2í•™ë…„ë¶€í„° ì§€ì› ê°€ëŠ¥, ì—°ê°„ 20ëª… ì„ ë°œ
* **ë³µìˆ˜ì „ê³µ**: 2í•™ë…„ë¶€í„° ì‹ ì²­ ê°€ëŠ¥
* **í¸ì…í•™**: 3í•™ë…„ í¸ì… ëª¨ì§‘, ë§¤ë…„ 3ì›”

**ì¤‘ìš”í•œ ì§€ì¹¨:**
1. ì •í™•í•œ ë‚ ì§œëŠ” "YYYYë…„ MMì›” DDì¼(ìš”ì¼) HH:MM" í˜•ì‹ìœ¼ë¡œ ì‘ì„±
2. ê¸ˆì•¡ì€ ì •í™•í•œ ìˆ«ìì™€ ë‹¨ìœ„ë¡œ í‘œì‹œ (ì˜ˆ: 1,500,000ì›)
3. ì •ë³´ê°€ ì—†ìœ¼ë©´ "ì •ë³´ ì—†ìŒ" ë˜ëŠ” "ë¬¸ì„œì—ì„œ í™•ì¸ë˜ì§€ ì•ŠìŒ"ìœ¼ë¡œ í‘œì‹œ
4. ì¶”ì¸¡í•˜ì§€ ë§ê³  ë¬¸ì„œì— ëª…ì‹œëœ ë‚´ìš©ë§Œ ì‘ì„±
5. ì „í˜•ëª…, í•™ê³¼ëª…ì€ ë¬¸ì„œì˜ ì •í™•í•œ ëª…ì¹­ ì‚¬ìš©
6. ê° í•­ëª©ë³„ë¡œ ê°€ëŠ¥í•œ í•œ ìƒì„¸í•˜ê²Œ ì‘ì„±

**ë¶„ì„í•  í…ìŠ¤íŠ¸:**
{context}
""")
    ])

    # ë‹¨ê³„ 7: ì–¸ì–´ëª¨ë¸ ìƒì„± (temperature ì¡°ì •)
    llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0.1)

    # ë‹¨ê³„ 8: ì²´ì¸ ìƒì„±
    def format_docs(docs):
        return "\n\n".join(doc.page_content for doc in docs)

    # ê²€ìƒ‰ ì¿¼ë¦¬ë¥¼ í•µì‹¬ í•­ëª©ì— ë§ì¶° ê°œì„ 
    search_queries = [
        "ì…í•™ìê²© ì§€ì›ìê²© í•™ë ¥ì¡°ê±´ ì¡¸ì—… ì¬ì§ì",
        "ëª¨ì§‘ì •ì› ëª¨ì§‘ì¸ì› í•™ê³¼ë³„ ì •ì› ì„ ë°œì¸ì›",
        "ì…ì‹œì¼ì • ì›ì„œì ‘ìˆ˜ í•©ê²©ë°œí‘œ ë©´ì ‘ ê³ ì‚¬ì¼ ì‹œí—˜ì¼ì •",
        "ì „í˜•ë°©ë²• í‰ê°€ê¸°ì¤€ ìˆ˜ì‹œ ì •ì‹œ ì „í˜•ìœ í˜•",
        "ìˆ˜ëŠ¥ìµœì €í•™ë ¥ê¸°ì¤€ ìµœì €ë“±ê¸‰ ìˆ˜ëŠ¥ë“±ê¸‰ í•™ë ¥ê¸°ì¤€",
        "ì œì¶œì„œë¥˜ êµ¬ë¹„ì„œë¥˜ í•„ìš”ì„œë¥˜ ì¦ëª…ì„œ ì¶”ì²œì„œ",
        "ë“±ë¡ê¸ˆ í•™ë¹„ ìˆ˜ì—…ë£Œ ë‚©ì…ê¸ˆ",
        "ì¥í•™ê¸ˆ ì§€ì›ê¸ˆ ê°ë©´ í˜œíƒ ì„±ì ìš°ìˆ˜",
        "ìœ ì˜ì‚¬í•­ ì£¼ì˜ì‚¬í•­ ì¤‘ë³µì§€ì› ì œí•œì‚¬í•­",
        "ê¸°ìˆ™ì‚¬ êµí™˜í•™ìƒ íŠ¹ë³„ì „í˜• í¸ì…í•™ ë³µìˆ˜ì „ê³µ"
    ]
    
    # ëª¨ë“  ê²€ìƒ‰ ê²°ê³¼ í†µí•©
    all_docs = []
    for query in search_queries:
        docs = retriever.invoke(query)
        all_docs.extend(docs)
    
    # ì¤‘ë³µ ì œê±° (ë‚´ìš© ê¸°ì¤€)
    unique_docs = []
    seen_content = set()
    for doc in all_docs:
        if doc.page_content not in seen_content:
            unique_docs.append(doc)
            seen_content.add(doc.page_content)
    
    context = format_docs(unique_docs)
    
    print(f"ê²€ìƒ‰ëœ ë¬¸ì„œ ì¡°ê° ìˆ˜: {len(unique_docs)}")
    print(f"ì»¨í…ìŠ¤íŠ¸ í† í° ìˆ˜: {count_tokens(context)}")

    # ì²´ì¸ ì‹¤í–‰
    chain = prompt | llm | StrOutputParser()
    
    try:
        summary_res = chain.invoke({"context": context})
        print("\n" + "="*50)
        print("ğŸ“Š ì…ì‹œìš”ê°• ìš”ì•½ ê²°ê³¼")
        print("="*50)
        print(summary_res)
        print("="*50)
        
        return summary_res
    except Exception as e:
        print(f"ìš”ì•½ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None

def process_files(input_folder):
    input_folder = Path(input_folder)
    output_folder = input_folder / "summaries"
    output_folder.mkdir(exist_ok=True)

    pdf_files = list(input_folder.glob("*.pdf"))
    if not pdf_files:
        print("PDF íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return

    for file_path in pdf_files:
        print(f"\n{'='*60}")
        print(f"ğŸ“„ {file_path.name} ì²˜ë¦¬ ì¤‘...")
        print('='*60)
        
        try:
            summary = process_documents(file_path)
            if summary:
                # ëŒ€í•™ ì´ë¦„ ì¶”ì¶œ (íŒŒì¼ëª…ì—ì„œ)
                university_name = file_path.stem
                
                # TXT íŒŒì¼ë¡œë„ ì €ì¥ (ê¸°ì¡´ ê¸°ëŠ¥ ìœ ì§€)
                txt_output_file = output_folder / f"{file_path.stem}_ìš”ì•½.txt"
                with open(txt_output_file, "w", encoding="utf-8") as f:
                    f.write(f"ğŸ« {file_path.name} ì…ì‹œìš”ê°• ìš”ì•½\n")
                    f.write("="*60 + "\n\n")
                    f.write(summary)
                print(f"âœ… TXT ìš”ì•½ ì™„ë£Œ: {txt_output_file}")
                
                # PDF íŒŒì¼ë¡œ ì €ì¥ (ìƒˆë¡œìš´ ê¸°ëŠ¥)
                pdf_output_file = output_folder / f"{file_path.stem}_ìš”ì•½.pdf"
                if create_pdf_summary(summary, pdf_output_file, university_name):
                    print(f"âœ… PDF ìš”ì•½ ì™„ë£Œ: {pdf_output_file}")
                else:
                    print(f"âŒ PDF ìƒì„± ì‹¤íŒ¨: {pdf_output_file}")
                    
            else:
                print(f"âŒ {file_path.name} ìš”ì•½ ì‹¤íŒ¨")
        except Exception as e:
            print(f"âŒ {file_path.name} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")

if __name__ == "__main__":
    # í™˜ê²½ë³€ìˆ˜ì—ì„œ OpenAI API í‚¤ í™•ì¸
    if not os.getenv("OPENAI_API_KEY"):
        print("âš ï¸  OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")
        exit(1)
    
    input_folder = r"C:\ì…ì‹œ-pdf-ìš”ì•½-í…ŒìŠ¤íŠ¸"
    
    if not Path(input_folder).exists():
        print(f"âŒ í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {input_folder}")
        exit(1)
    
    process_files(input_folder)
    print("\nğŸ‰ ëª¨ë“  ì‘ì—… ì™„ë£Œ!")
