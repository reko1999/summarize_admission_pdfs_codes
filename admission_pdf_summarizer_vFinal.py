# ì…ì‹œ ìš”ê°• PDF ìš”ì•½ ì‹œìŠ¤í…œ - LangChain í™œìš©
from pathlib import Path

from langchain.vectorstores import FAISS
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.chains import RetrievalQA
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain.prompts import PromptTemplate
from langchain_community.document_loaders import PDFPlumberLoader

# from langchain_summarize_test_2 import save_extracted_docs

import re
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

DEBUG_TEXT = ""

class AdmissionSummarizer:
    def __init__(self, openai_api_key):
        self.embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)
        self.llm = ChatOpenAI(
            model_name = "gpt-4o",
            temperature=0,
            openai_api_key=openai_api_key
        )
        self.vectorstore = None

        # 1. ì„¹ì…˜ë³„ ê²€ìƒ‰ ì¿¼ë¦¬ ì •ì˜
        self.section_queries = {
            "ì…í•™ìê²©": [
                "ì…í•™ ì§€ì› ìê²©ê³¼ í•™ë ¥ ì¡°ê±´ì€?",
                "ì¬ì§ì íŠ¹ë³„ì „í˜• ì§€ì› ìš”ê±´ì€?",
                "ì™¸êµ­ì¸ íŠ¹ë³„ì „í˜• ìê²© ê¸°ì¤€ì€?",
                "ì„±ì¸í•™ìŠµì ì…í•™ ìê²©ì€?",
                "í¸ì…í•™ ì§€ì› ìê²© ì¡°ê±´ì€?"
            ],
            "ëª¨ì§‘ì •ì›": [
                "í•™ê³¼ë³„ ëª¨ì§‘ì •ì›ì€ ëª‡ ëª…ì¸ê°€?",
                "ìˆ˜ì‹œ ì •ì‹œ ì „í˜•ë³„ ëª¨ì§‘ì¸ì›ì€?",
                "ì •ì›ë‚´ ì •ì›ì™¸ ëª¨ì§‘ êµ¬ë¶„ê³¼ ì¸ì›ì€?",
                "íŠ¹ë³„ì „í˜•ë³„ ì„ ë°œ ì¸ì›ì€?",
                "ì´ ëª¨ì§‘ì •ì›ê³¼ í•™ë¶€ë³„ ë°°ì • ì¸ì›ì€?"
            ],
            "ì…ì‹œì¼ì •": [
                "ì›ì„œì ‘ìˆ˜ ê¸°ê°„ì€ ì–¸ì œì¸ê°€?",
                "ë©´ì ‘ê³¼ ì‹¤ê¸°ê³ ì‚¬ ì¼ì •ì€?",
                "í•©ê²©ë°œí‘œì¼ì€ ì–¸ì œì¸ê°€?",
                "ë“±ë¡ ë° ì¶”ê°€í•©ê²© ì¼ì •ì€?",
                "ì „í˜•ë³„ ì£¼ìš” ì¼ì • ì°¨ì´ëŠ”?",
            ],
            "ì „í˜•ë°©ë²•": [
                "ìˆ˜ì‹œì „í˜• í‰ê°€ë°©ë²•ê³¼ ë°˜ì˜ë¹„ìœ¨ì€?",
                "ì •ì‹œì „í˜• í‰ê°€ê¸°ì¤€ê³¼ ë¹„ìœ¨ì€?",
                "ë©´ì ‘ê³ ì‚¬ í‰ê°€ ë°©ë²•ê³¼ ë¹„ì¤‘ì€?",
                "ì‹¤ê¸°ê³ ì‚¬ í‰ê°€ ê¸°ì¤€ì€?",
                "í•™ìƒë¶€ ë°˜ì˜ë°©ë²•ê³¼ ë¹„ìœ¨ì€?"
            ],
            "ìˆ˜ëŠ¥ìµœì €": [
                "ìˆ˜ëŠ¥ ìµœì €í•™ë ¥ê¸°ì¤€ì€?",
                "ì¸ë¬¸ê³„ì—´ ìˆ˜ëŠ¥ ìµœì €ë“±ê¸‰ ê¸°ì¤€ì€?",
                "ìì—°ê³„ì—´ ìˆ˜ëŠ¥ ìµœì €ë“±ê¸‰ ê¸°ì¤€ì€?",
                "ì˜ˆì²´ëŠ¥ê³„ì—´ ìˆ˜ëŠ¥ ìµœì €ê¸°ì¤€ì€?",
                "ìˆ˜ëŠ¥ ë°˜ì˜ì˜ì—­ê³¼ ì„ íƒê³¼ëª©ì€?"
            ],
            "ì œì¶œì„œë¥˜": [
                "ê³µí†µ ì œì¶œì„œë¥˜ ëª©ë¡ì€?",
                "ìˆ˜ì‹œì „í˜• ì œì¶œì„œë¥˜ëŠ”?",
                "ì •ì‹œì „í˜• ì œì¶œì„œë¥˜ëŠ”?",
                "íŠ¹ë³„ì „í˜• ì¶”ê°€ ì œì¶œì„œë¥˜ëŠ”?",
                "ì„œë¥˜ ì œì¶œë°©ë²•ê³¼ ìœ ì˜ì‚¬í•­ì€?"
            ],
            "ë“±ë¡ê¸ˆ": [
                "ì¸ë¬¸ì‚¬íšŒê³„ì—´ ë“±ë¡ê¸ˆì€?",
                "ìì—°ê³µí•™ê³„ì—´ ë“±ë¡ê¸ˆì€?",
                "ì˜ˆì²´ëŠ¥ê³„ì—´ ë“±ë¡ê¸ˆì€?",
                "ì…í•™ê¸ˆê³¼ ìˆ˜ì—…ë£ŒëŠ” ê°ê° ì–¼ë§ˆì¸ê°€?",
                "ê¸°íƒ€ ë‚©ë¶€ê¸ˆê³¼ ì´ ë“±ë¡ë¹„ìš©ì€?"
            ],
            "ì¥í•™ê¸ˆ": [
                "ì‹ ì…ìƒ ì¥í•™ê¸ˆ ì¢…ë¥˜ì™€ ì¡°ê±´ì€?",
                "ì„±ì ìš°ìˆ˜ ì¥í•™ê¸ˆ ê¸°ì¤€ê³¼ ì§€ì›ê·œëª¨ëŠ”?",
                "ì†Œë“ì—°ê³„ ì¥í•™ê¸ˆ ì¡°ê±´ì€?",
                "íŠ¹ë³„ì „í˜• ì¥í•™ê¸ˆ í˜œíƒì€?",
                "ì¥í•™ê¸ˆ ì¤‘ë³µìˆ˜í˜œì™€ ìœ ì§€ì¡°ê±´ì€?"
            ],
            "ìœ ì˜ì‚¬í•­": [
                "ì¤‘ë³µì§€ì› ê¸ˆì§€ ë° ì œí•œì‚¬í•­ì€?",
                "ì›ì„œì ‘ìˆ˜ ë° ì„œë¥˜ì œì¶œ ì£¼ì˜ì‚¬í•­ì€?",
                "ì „í˜•ë£Œ ë‚©ë¶€ì™€ í™˜ë¶ˆ ê·œì •ì€?",
                "í•©ê²©ì ë“±ë¡ ê´€ë ¨ ìœ ì˜ì‚¬í•­ì€?",
                "ê¸°íƒ€ ì…ì‹œ ê´€ë ¨ ì£¼ì˜ì‚¬í•­ì€?"
            ],
            "ê¸°íƒ€ì‚¬í•­": [
                "ê¸°ìˆ™ì‚¬ ì‹ ì²­ë°©ë²•ê³¼ ë¹„ìš©ì€?",
                "êµí™˜í•™ìƒ í”„ë¡œê·¸ë¨ ì •ë³´ëŠ”?",
                "í¸ì…í•™ ê´€ë ¨ ì¶”ê°€ ì •ë³´ëŠ”?",
                "ì‹ ì…ìƒ ì˜¤ë¦¬ì—”í…Œì´ì…˜ê³¼ ì§€ì›ì œë„ëŠ”?",
                "í•™ì‚¬ ìš´ì˜ê³¼ ìº í¼ìŠ¤ ì‹œì„¤ ì •ë³´ëŠ”?"
            ]
        }

        # 2. ì„¹ì…˜ë³„ ìƒì„¸ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
        self.section_prompts = {
            "ì…í•™ìê²©": """
                ë‹¤ìŒ ì…ì‹œ ìš”ê°• ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì…í•™ìê²© ì •ë³´ë¥¼ ì •í™•í•˜ê³  ì²´ê³„ì ìœ¼ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”.

                **ìš”ì•½ ê¸°ì¤€:**
                - ì¼ë°˜ì „í˜•, íŠ¹ë³„ì „í˜•ë³„ í•™ë ¥ ì¡°ê±´
                - ì¬ì§ì/ì™¸êµ­ì¸ ë“± íŠ¹ë³„ ìš”ê±´
                - í¸ì…í•™ ìê²© ì¡°ê±´
                - ì§€ì› ì œí•œ ì‚¬í•­

                **ê´€ë ¨ ì •ë³´:**
                {context}

                **ì…í•™ìê²© ìš”ì•½:**
                """,

            "ëª¨ì§‘ì •ì›": """
                ë‹¤ìŒ ì…ì‹œ ìš”ê°• ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ëª¨ì§‘ì •ì› ì •ë³´ë¥¼ ì •í™•í•˜ê³  ì²´ê³„ì ìœ¼ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”.

                **ìš”ì•½ ê¸°ì¤€:**
                - ê³„ì—´ë³„ ëª¨ì§‘ì¸ì›
                - ì „í˜•ë³„ ì„¸ë¶€ ì¸ì›
                - ì •ì› ë‚´/ì™¸ êµ¬ë¶„
                - í•™ê³¼ë³„ ì •í™•í•œ ëª¨ì§‘ì¸ì›

                **ê´€ë ¨ ì •ë³´:**
                {context}

                **ëª¨ì§‘ì •ì› ìš”ì•½:**
                """,

            "ì…ì‹œì¼ì •": """
                ë‹¤ìŒ ì…ì‹œ ìš”ê°• ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì…ì‹œì¼ì •ì„ ì‹œê°„ìˆœìœ¼ë¡œ ì •í™•í•˜ê²Œ ìš”ì•½í•´ì£¼ì„¸ìš”.

                **ìš”ì•½ ê¸°ì¤€:**
                - ì›ì„œì ‘ìˆ˜ ~ í•©ê²©ë°œí‘œê¹Œì§€ ì‹œê°„ìˆœ ì •ë¦¬
                - ê° ì „í˜•ë³„ ì¼ì • êµ¬ë¶„
                - ì¤‘ìš” ë§ˆê°ì¼ê³¼ ì£¼ì˜ì‚¬í•­
                - ë©´ì ‘/ì‹¤ê¸° ë“± ê°œë³„ ì¼ì •

                **ê´€ë ¨ ì •ë³´:**
                {context}

                **ì…ì‹œì¼ì • ìš”ì•½:**
                """,

            "ì „í˜•ë°©ë²•": """
                ë‹¤ìŒ ì…ì‹œ ìš”ê°• ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì „í˜•ë°©ë²•ì„ ì •í™•í•˜ê³  ì²´ê³„ì ìœ¼ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”.

                **ìš”ì•½ ê¸°ì¤€:**
                - ìˆ˜ì‹œ/ì •ì‹œë³„ í‰ê°€ ë°©ë²•
                - í•™ìƒë¶€/ìˆ˜ëŠ¥/ë©´ì ‘ ë“± ë°˜ì˜ ë¹„ìœ¨
                - ì„œë¥˜í‰ê°€ ê¸°ì¤€
                - ì‹¤ê¸°/ë©´ì ‘ í‰ê°€ ë°©ë²•

                **ê´€ë ¨ ì •ë³´:**
                {context}

                **ì „í˜•ë°©ë²• ìš”ì•½:**
                """,

            "ìˆ˜ëŠ¥ìµœì €": """
                ë‹¤ìŒ ì…ì‹œ ìš”ê°• ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìˆ˜ëŠ¥ ìµœì €í•™ë ¥ê¸°ì¤€ì„ ì •í™•í•˜ê³  ì²´ê³„ì ìœ¼ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”.

                **ìš”ì•½ ê¸°ì¤€:**
                - í•™ê³¼ë³„/ëª¨ì§‘ë‹¨ìœ„ë³„ ë“±ê¸‰ ìš”ê±´
                - ë°˜ì˜ ì˜ì—­ê³¼ ì¡°í•©
                - íŠ¹ë³„ì „í˜•ë³„ ê¸°ì¤€
                - í•œêµ­ì‚¬/íƒêµ¬ ë“± ì„¸ë¶€ ì¡°ê±´

                **ê´€ë ¨ ì •ë³´:**
                {context}

                **ìˆ˜ëŠ¥ ìµœì €í•™ë ¥ê¸°ì¤€ ìš”ì•½:**
                """,

            "ì œì¶œì„œë¥˜": """
                ë‹¤ìŒ ì…ì‹œ ìš”ê°• ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì œì¶œì„œë¥˜ë¥¼ ì •í™•í•˜ê³  ì²´ê³„ì ìœ¼ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”.

                **ìš”ì•½ ê¸°ì¤€:**
                - ì „í˜•ë³„ í•„ìˆ˜ ì œì¶œì„œë¥˜
                - ì„ íƒ ì œì¶œì„œë¥˜
                - ì„œë¥˜ ì œì¶œ ë°©ë²•ê³¼ ê¸°í•œ
                - ì¶”ê°€ ì„œë¥˜ ìš”êµ¬ì‚¬í•­

                **ê´€ë ¨ ì •ë³´:**
                {context}

                **ì œì¶œì„œë¥˜ ìš”ì•½:**
                """,

            "ë“±ë¡ê¸ˆ": """
                ë‹¤ìŒ ì…ì‹œ ìš”ê°• ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë“±ë¡ê¸ˆ ì •ë³´ë¥¼ ì •í™•í•˜ê³  ì²´ê³„ì ìœ¼ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”.

                **ìš”ì•½ ê¸°ì¤€:**
                - í•™ê³¼/ê³„ì—´ë³„ ì •í™•í•œ ë“±ë¡ê¸ˆ ê¸ˆì•¡
                - ì…í•™ê¸ˆê³¼ ìˆ˜ì—…ë£Œ êµ¬ë¶„
                - ì¶”ê°€ ë‚©ë¶€ ë¹„ìš©
                - ë“±ë¡ê¸ˆ ë‚©ë¶€ ë°©ë²•ê³¼ ê¸°í•œ

                **ê´€ë ¨ ì •ë³´:**
                {context}

                **ë“±ë¡ê¸ˆ ì •ë³´ ìš”ì•½:**
                """,

            "ì¥í•™ê¸ˆ": """
                ë‹¤ìŒ ì…ì‹œ ìš”ê°• ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¥í•™ê¸ˆ ì •ë³´ë¥¼ ì •í™•í•˜ê³  ì²´ê³„ì ìœ¼ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”.

                **ìš”ì•½ ê¸°ì¤€:**
                - ì¥í•™ê¸ˆ ì¢…ë¥˜ë³„ ì¡°ê±´ê³¼ ê¸ˆì•¡
                - ì„±ì /ì†Œë“ ê¸°ì¤€
                - ì‹ ì²­ ë°©ë²•ê³¼ ê¸°í•œ
                - ì§€ì› ê·œëª¨ì™€ í˜œíƒ

                **ê´€ë ¨ ì •ë³´:**
                {context}

                **ì¥í•™ê¸ˆ ì •ë³´ ìš”ì•½:**
                """,

            "ìœ ì˜ì‚¬í•­": """
                ë‹¤ìŒ ì…ì‹œ ìš”ê°• ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìœ ì˜ì‚¬í•­ì„ ì •í™•í•˜ê³  ì²´ê³„ì ìœ¼ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”.

                **ìš”ì•½ ê¸°ì¤€:**
                - ì¤‘ë³µì§€ì› ì œí•œ ì‚¬í•­
                - ë§ˆê°ì¼ê³¼ ì‹œê°„ ê´€ë ¨ ì£¼ì˜ì‚¬í•­
                - ì „í˜•ë³„ íŠ¹ë³„ ìœ ì˜ì‚¬í•­
                - ê¸°íƒ€ ì¤‘ìš”í•œ ì œí•œì‚¬í•­

                **ê´€ë ¨ ì •ë³´:**
                {context}

                **ìœ ì˜ì‚¬í•­ ìš”ì•½:**
                """,

            "ê¸°íƒ€ì‚¬í•­": """
                ë‹¤ìŒ ì…ì‹œ ìš”ê°• ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê¸°íƒ€ ì¤‘ìš”ì‚¬í•­ì„ ì •í™•í•˜ê³  ì²´ê³„ì ìœ¼ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”.

                **ìš”ì•½ ê¸°ì¤€:**
                - ê¸°ìˆ™ì‚¬ ì •ë³´ì™€ ì‹ ì²­ ë°©ë²•
                - êµí™˜í•™ìƒ/í•´ì™¸ì—°ìˆ˜ í”„ë¡œê·¸ë¨
                - í¸ì…í•™ ê´€ë ¨ ì •ë³´
                - ê¸°íƒ€ í•™êµ í˜œíƒê³¼ í”„ë¡œê·¸ë¨

                **ê´€ë ¨ ì •ë³´:**
                {context}

                **ê¸°íƒ€ ì¤‘ìš”ì‚¬í•­ ìš”ì•½:**
                """
        }

        # 3. ìµœì¢… í†µí•© í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
        self.final_prompt = PromptTemplate(
            input_variables=["sections"],
            template="""
            ë‹¤ìŒì€ ì…ì‹œ ìš”ê°•ì—ì„œ ì¶”ì¶œí•œ ì„¹ì…˜ë³„ ì •ë³´ì…ë‹ˆë‹¤. 
            ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì™„ì „í•˜ê³  ì²´ê³„ì ì¸ ì…ì‹œ ìš”ê°• ìš”ì•½ì„œë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.

            {sections}
            
            **ìµœì¢… ì…ì‹œ ìš”ê°• ìš”ì•½ì„œ:**

            # ğŸ“ ì…í•™ ìê²©
            {ì…í•™ìê²© ë‚´ìš©ì„ ì—¬ê¸°ì— ì •ë¦¬}

            # ğŸ‘¥ ëª¨ì§‘ ì •ì›
            {ëª¨ì§‘ì •ì› ë‚´ìš©ì„ ì—¬ê¸°ì— ì •ë¦¬}

            # ğŸ“‹ ì…ì‹œ ì¼ì •
            {ì…ì‹œì¼ì • ë‚´ìš©ì„ ì—¬ê¸°ì— ì •ë¦¬}

            # ğŸ“š ì „í˜• ë°©ë²•
            {ì „í˜•ë°©ë²• ë‚´ìš©ì„ ì—¬ê¸°ì— ì •ë¦¬}

            # ğŸ“Š ìˆ˜ëŠ¥ ìµœì €í•™ë ¥ê¸°ì¤€
            {ìˆ˜ëŠ¥ìµœì € ë‚´ìš©ì„ ì—¬ê¸°ì— ì •ë¦¬}

            # ğŸ“„ ì œì¶œ ì„œë¥˜
            {ì œì¶œì„œë¥˜ ë‚´ìš©ì„ ì—¬ê¸°ì— ì •ë¦¬}

            # ğŸ’° ë“±ë¡ê¸ˆ ì •ë³´
            {ë“±ë¡ê¸ˆ ë‚´ìš©ì„ ì—¬ê¸°ì— ì •ë¦¬}

            # ğŸ“ ì¥í•™ê¸ˆ ì •ë³´
            {ì¥í•™ê¸ˆ ë‚´ìš©ì„ ì—¬ê¸°ì— ì •ë¦¬}

            # âš ï¸ ìœ ì˜ì‚¬í•­
            {ìœ ì˜ì‚¬í•­ ë‚´ìš©ì„ ì—¬ê¸°ì— ì •ë¦¬}

            # ğŸ“Œ ê¸°íƒ€ ì¤‘ìš”ì‚¬í•­
            {ê¸°íƒ€ì‚¬í•­ ë‚´ìš©ì„ ì—¬ê¸°ì— ì •ë¦¬}
            """
        )

    def load_and_process_pdf(self, pdf_path):
        """PDF ë¡œë“œ ë° ë²¡í„°ìŠ¤í† ì–´ ìƒì„±"""
        # PDF ë¡œë“œ
        loader = PDFPlumberLoader(pdf_path)
        documents = loader.load()

        # í…ìŠ¤íŠ¸ ë¶„í•  (ì…ì‹œ ìš”ê°•ì— ë§ê²Œ ì¡°ì •)
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1100,
            chunk_overlap=300,
            separators=["\n\n", "\n", ".", " ", ""]
        )
        docs = text_splitter.split_documents(documents)
        print(f"ë¶„í• ëœ ë¬¸ì„œ ì¡°ê° ìˆ˜: {len(docs)}")

        # chunk_index = 0
        # for chunk in docs:
        #     chunk_index += 1
        #     print(f"chunk.page_content[{chunk_index}] ê¸¸ì´: {len(chunk.page_content)}")
        #     print(f"chunk.page_content[{chunk_index}]:")
        #     print(chunk.page_content)

        self.save_extracted_docs(pdf_path, docs)

        # ë²¡í„°ìŠ¤í† ì–´ ìƒì„±
        self.vectorstore = FAISS.from_documents(docs, self.embeddings)

    def save_extracted_docs(self, pdf_path, docs):
        debug_file = Path(pdf_path).parent / f"{Path(pdf_path).stem}_extracted_docs.txt"
        global DEBUG_TEXT
        with open(debug_file, "w", encoding="utf-8") as f:
            for doc in docs:
                DEBUG_TEXT+= doc.page_content + '\n'
                f.write(doc.page_content + '\n')

    def retrieve_section_info(self, section_name):
        """ì„¹ì…˜ë³„ ì •ë³´ ê²€ìƒ‰ ë° ìš”ì•½"""
        if not self.vectorstore:
            raise ValueError("PDFë¥¼ ë¨¼ì € ë¡œë“œí•´ì£¼ì„¸ìš”.")

        section_info = []
        queries = self.section_queries.get(section_name, [])

        # ì—¬ëŸ¬ ì¿¼ë¦¬ë¡œ ê²€ìƒ‰í•˜ì—¬ ì •ë³´ ìˆ˜ì§‘
        for i, query in enumerate(queries):
            print(f"\n{'=' * 50}")
            print(f"Query {i + 1}: {query}")
            print(f"{'=' * 50}")
            retriever = self.vectorstore.as_retriever(
                search_type="similarity_score_threshold",
                search_kwargs={
                    "score_threshold": 0.75,
                    "k": 7
                }
            )
            docs = retriever.invoke(query)
            # docs = self.vectorstore.similarity_search_with_score(
            #     query,
            #     k=10  # ë” ë§ì´ ê°€ì ¸ì™€ì„œ threshold ì ìš© í›„ ìƒìœ„ 5ê°œ ì„ íƒ
            # )
            print(f"ê²€ìƒ‰ëœ ë¬¸ì„œ ê°œìˆ˜: {len(docs)}")

            # ê° ë¬¸ì„œì˜ ìœ ì‚¬ë„ ì ìˆ˜ì™€ ë‚´ìš© í™•ì¸
            # for j, doc in enumerate(docs):
            #     print(f"\n--- ë¬¸ì„œ {j + 1} ---")
            #
            #     # ë©”íƒ€ë°ì´í„°ì—ì„œ ìœ ì‚¬ë„ ì ìˆ˜ í™•ì¸ (vectorstoreì— ë”°ë¼ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ)
            #     if hasattr(doc, 'metadata') and 'score' in doc.metadata:
            #         print(f"ìœ ì‚¬ë„ ì ìˆ˜: {doc.metadata['score']:.4f}")
            #
            #     # ë¬¸ì„œ ë‚´ìš© ë¯¸ë¦¬ë³´ê¸° (ì²˜ìŒ 200ì)
            #     # content_preview = doc.page_content[:200] + "..." if len(doc.page_content) > 200 else doc.page_content
            #     content_preview = doc.page_content
            #     print(f"ë¬¸ì„œ ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°: {content_preview}")

            section_info.extend([doc.page_content for doc in docs])
            print(f"\ní˜„ì¬ê¹Œì§€ ìˆ˜ì§‘ëœ ì´ ì„¹ì…˜ ìˆ˜: {len(section_info)}")

        # ì¤‘ë³µ ì œê±°
        unique_info = list(set(section_info))
        context = "\n\n".join(unique_info)
        print("context:", context)

        # ì„¹ì…˜ë³„ ìš”ì•½ ìƒì„±
        prompt = self.section_prompts.get(section_name, "")
        if prompt:
            formatted_prompt = prompt.format(context=context)
            response = self.llm(formatted_prompt)

            # content ì†ì„±ì´ ìˆëŠ”ì§€ í™•ì¸ í›„ ì¶”ì¶œ
            if hasattr(response, 'content'):
                summary = response.content
            else:
                summary = str(response)  # í˜¹ì‹œ ëª¨ë¥¼ ê²½ìš° ëŒ€ë¹„

            return summary

        return context

        # return 1

    def generate_full_summary(self, pdf_path):
        """ì „ì²´ ì…ì‹œ ìš”ê°• ìš”ì•½ ìƒì„±"""
        # PDF ì²˜ë¦¬
        self.load_and_process_pdf(pdf_path)

        # ì„¹ì…˜ë³„ ìš”ì•½ ìƒì„±
        sections = {}
        for section_name in self.section_queries.keys():
            print(f"Processing {section_name}...")
            sections[section_name] = self.retrieve_section_info(section_name)
        # for section_name in self.section_queries.keys():
        #     print("section name:", section_name)
        #     print(f"sections[{section_name}]: {sections[section_name]}")

        return sections

        # # ìµœì¢… ìš”ì•½ì„œ ìƒì„±
        # sections_text = "\n\n".join([
        #     f"**{name}:**\n{content}"
        #     for name, content in sections.items()
        # ])
        #
        # print("sections_text:")
        # print(sections_text)
        #
        # return sections_text

        # final_summary = self.final_prompt.format(sections=sections_text)
        # return self.llm(final_summary)

        # return 1


class SimpleSummaryEvaluator:

    def evaluate(self, original_text, summary_text):
        """
        4ê°€ì§€ í•µì‹¬ ì§€í‘œë¡œ ìš”ì•½ í’ˆì§ˆ í‰ê°€
        """
        results = {
            'semantic_similarity': self.semantic_similarity(original_text, summary_text),
            'info_preservation': self.info_preservation(original_text, summary_text),
            'readability': self.readability(summary_text),
            'compression_efficiency': self.compression_efficiency(original_text, summary_text)
        }

        # ì¢…í•© ì ìˆ˜ (ì •ë³´ ë³´ì¡´ìœ¨ì— ë†’ì€ ê°€ì¤‘ì¹˜)
        weights = [0.25, 0.40, 0.20, 0.15]  # ì˜ë¯¸ìœ ì‚¬ë„, ì •ë³´ë³´ì¡´, ê°€ë…ì„±, ì••ì¶•íš¨ìœ¨
        results['overall_score'] = sum(score * weight for score, weight in zip(results.values(), weights))

        return results

    def semantic_similarity(self, original, summary):
        """ì˜ë¯¸ì  ìœ ì‚¬ë„ (TF-IDF ì½”ì‚¬ì¸ ìœ ì‚¬ë„)"""
        try:
            texts = [original, summary]
            vectorizer = TfidfVectorizer(max_features=500, ngram_range=(1, 2))
            vectors = vectorizer.fit_transform(texts)
            similarity = cosine_similarity(vectors[0:1], vectors[1:2])[0][0]
            return similarity * 100
        except:
            return 0

    def info_preservation(self, original, summary):
        """í•µì‹¬ ì •ë³´ ë³´ì¡´ìœ¨ (TF-IDF ê¸°ë°˜)"""
        try:
            # ì›ë³¸ì—ì„œ ìƒìœ„ ì¤‘ìš” ë‹¨ì–´ ì¶”ì¶œ
            vectorizer = TfidfVectorizer(max_features=30, stop_words=None)
            original_vector = vectorizer.fit_transform([original])

            # ì¤‘ìš” ë‹¨ì–´ë“¤
            feature_names = vectorizer.get_feature_names_out()
            important_words = set(feature_names)

            # ìš”ì•½ì—ì„œ ë‹¨ì–´ ì¶”ì¶œ
            summary_words = set(summary.split())

            # ë³´ì¡´ëœ ì¤‘ìš” ë‹¨ì–´ ë¹„ìœ¨
            preserved_words = important_words & summary_words
            preservation_ratio = len(preserved_words) / len(important_words) if important_words else 0

            return preservation_ratio * 100

        except:
            return 0

    def readability(self, text):
        """ê°€ë…ì„± ì ìˆ˜"""
        sentences = [s.strip() for s in re.split(r'[.!?]', text) if s.strip()]

        if not sentences:
            return 0

        # í‰ê·  ë¬¸ì¥ ê¸¸ì´ (í•œêµ­ì–´ ì ì •: 15-25ì)
        avg_length = sum(len(s) for s in sentences) / len(sentences)
        length_score = max(0, 100 - abs(avg_length - 20) * 3)

        # ë³µì¡ë„ (ì ‘ì†ì‚¬ ì‚¬ìš©ë¥ )
        conjunctions = ['ê·¸ë¦¬ê³ ', 'í•˜ì§€ë§Œ', 'ê·¸ëŸ¬ë‚˜', 'ë˜í•œ', 'ë”°ë¼ì„œ']
        complex_ratio = sum(1 for s in sentences if any(c in s for c in conjunctions)) / len(sentences)
        complexity_score = max(0, 100 - abs(complex_ratio - 0.3) * 200)

        return (length_score + complexity_score) / 2

    def compression_efficiency(self, original, summary):
        """ì••ì¶• íš¨ìœ¨ì„± (ì ì • ì••ì¶•ë¥ : 20-40%)"""
        ratio = len(summary) / len(original) if len(original) > 0 else 0

        if 0.2 <= ratio <= 0.4:
            return 100
        elif ratio < 0.2:
            return 60  # ê³¼ë„í•œ ì••ì¶•
        else:
            return max(0, 100 - (ratio - 0.4) * 150)  # ì••ì¶• ë¶€ì¡±


def test_evaluator(original, summary):
    evaluator = SimpleSummaryEvaluator()

    results = evaluator.evaluate(original, summary)

    print("=== ìš”ì•½ í’ˆì§ˆ í‰ê°€ ===")
    print(f"ì˜ë¯¸ì  ìœ ì‚¬ë„: {results['semantic_similarity']:.1f}ì ")
    print(f"ì •ë³´ ë³´ì¡´ìœ¨: {results['info_preservation']:.1f}ì ")
    print(f"ê°€ë…ì„±: {results['readability']:.1f}ì ")
    print(f"ì••ì¶• íš¨ìœ¨ì„±: {results['compression_efficiency']:.1f}ì ")
    print(f"ì¢…í•© ì ìˆ˜: {results['overall_score']:.1f}ì ")
    print()


import os
import re
from reportlab.lib.pagesizes import A4
from reportlab.lib.units import mm
from reportlab.lib.colors import HexColor, white
from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle
from reportlab.lib.enums import TA_CENTER, TA_LEFT, TA_JUSTIFY
from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer
from reportlab.pdfbase import pdfmetrics
from reportlab.pdfbase.ttfonts import TTFont


class SimpleAdmissionPDFGenerator:
    def __init__(self, font_path=None):
        """
        ì…ì‹œìš”ê°• PDF ìƒì„±ê¸° ì´ˆê¸°í™” (í‘œ ìƒì„± ì—†ìŒ)

        Args:
            font_path (str): í•œêµ­ì–´ í°íŠ¸ íŒŒì¼ ê²½ë¡œ (ì˜µì…˜)
        """
        self.font_path = font_path
        self.setup_fonts()

        # ìƒ‰ìƒ ì •ì˜
        self.colors = {
            'primary': HexColor('#2E5984'),  # ë„¤ì´ë¹„ ë¸”ë£¨
            'secondary': HexColor('#4A90E2'),  # ë¼ì´íŠ¸ ë¸”ë£¨
            'accent': HexColor('#F8F9FA'),  # ì—°í•œ íšŒìƒ‰
            'text': HexColor('#2C3E50'),  # ë‹¤í¬ ê·¸ë ˆì´
            'header': HexColor('#1A365D'),  # í—¤ë” ìƒ‰ìƒ
        }

        # ìŠ¤íƒ€ì¼ ì„¤ì •
        self.setup_styles()

    def setup_fonts(self):
        """í°íŠ¸ ì„¤ì •"""
        try:
            if self.font_path and os.path.exists(self.font_path):
                pdfmetrics.registerFont(TTFont('Korean', self.font_path))
                self.font_name = 'Korean'
            else:
                # ê¸°ë³¸ í•œêµ­ì–´ í°íŠ¸ ê²½ë¡œë“¤ ì‹œë„
                font_paths = [
                    '/System/Library/Fonts/AppleSDGothicNeo.ttc',  # macOS
                    'C:/Windows/Fonts/malgun.ttf',  # Windows
                    '/usr/share/fonts/truetype/nanum/NanumGothic.ttf',  # Linux
                ]

                for path in font_paths:
                    if os.path.exists(path):
                        pdfmetrics.registerFont(TTFont('Korean', path))
                        self.font_name = 'Korean'
                        break
                else:
                    self.font_name = 'Helvetica'
                    print("í•œêµ­ì–´ í°íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ ê¸°ë³¸ í°íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        except Exception as e:
            self.font_name = 'Helvetica'
            print(f"í°íŠ¸ ì„¤ì • ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

    def setup_styles(self):
        """ë¬¸ì„œ ìŠ¤íƒ€ì¼ ì„¤ì •"""
        self.styles = getSampleStyleSheet()

        # ì œëª© ìŠ¤íƒ€ì¼
        self.styles.add(ParagraphStyle(
            name='CustomTitle',
            parent=self.styles['Title'],
            fontName=self.font_name,
            fontSize=24,
            textColor=self.colors['header'],
            spaceAfter=30,
            alignment=TA_CENTER,
            borderWidth=2,
            borderColor=self.colors['primary'],
            borderPadding=10,
            backColor=self.colors['accent']
        ))

        # ì„¹ì…˜ í—¤ë” ìŠ¤íƒ€ì¼
        self.styles.add(ParagraphStyle(
            name='SectionHeader',
            parent=self.styles['Heading1'],
            fontName=self.font_name,
            fontSize=16,
            textColor=white,
            backColor=self.colors['primary'],
            borderPadding=8,
            spaceAfter=12,
            spaceBefore=20,
            alignment=TA_LEFT
        ))

        # ë³¸ë¬¸ ìŠ¤íƒ€ì¼
        self.styles.add(ParagraphStyle(
            name='CustomBody',
            parent=self.styles['Normal'],
            fontName=self.font_name,
            fontSize=11,
            textColor=self.colors['text'],
            spaceAfter=8,
            alignment=TA_JUSTIFY,
            leftIndent=10,
            firstLineIndent=0,
            leading=16
        ))

        # ë¦¬ìŠ¤íŠ¸ ìŠ¤íƒ€ì¼
        self.styles.add(ParagraphStyle(
            name='ListItem',
            parent=self.styles['Normal'],
            fontName=self.font_name,
            fontSize=10,
            textColor=self.colors['text'],
            spaceAfter=6,
            leftIndent=20,
            bulletIndent=10,
            leading=14
        ))

        # ê°•ì¡° í…ìŠ¤íŠ¸ ìŠ¤íƒ€ì¼ (ì½œë¡ ì´ ìˆëŠ” ì •ë³´ì„± í…ìŠ¤íŠ¸ìš©)
        self.styles.add(ParagraphStyle(
            name='InfoText',
            parent=self.styles['Normal'],
            fontName=self.font_name,
            fontSize=10,
            textColor=self.colors['text'],
            spaceAfter=6,
            leftIndent=15,
            backColor=self.colors['accent'],
            borderPadding=5,
            leading=14
        ))

    def format_section_content(self, content):
        """ì„¹ì…˜ ë‚´ìš©ì„ í¬ë§·íŒ… (í‘œ ìƒì„± ì—†ìŒ)"""
        elements = []

        # ë‚´ìš©ì„ ë¬¸ìì—´ë¡œ ë³€í™˜
        content_str = str(content)
        lines = content_str.split('\n')
        current_paragraph = []

        for line in lines:
            line = line.strip()
            if not line:
                # ë¹ˆ ì¤„ì´ë©´ í˜„ì¬ ë¬¸ë‹¨ ì™„ë£Œ
                if current_paragraph:
                    elements.append(Paragraph(' '.join(current_paragraph), self.styles['CustomBody']))
                    current_paragraph = []
                continue

            # ë¦¬ìŠ¤íŠ¸ ì•„ì´í…œ ê°ì§€
            if (line.startswith('â€¢ ') or
                    line.startswith('- ') or
                    re.match(r'^\d+\.\s', line)):

                # ì´ì „ ë¬¸ë‹¨ ì™„ë£Œ
                if current_paragraph:
                    elements.append(Paragraph(' '.join(current_paragraph), self.styles['CustomBody']))
                    current_paragraph = []

                # ë¦¬ìŠ¤íŠ¸ ë§ˆì»¤ ì œê±°í•˜ê³  ì •ë¦¬
                clean_line = re.sub(r'^[â€¢\-]\s*|^\d+\.\s*', '', line).strip()
                if clean_line:
                    elements.append(Paragraph(f"â€¢ {clean_line}", self.styles['ListItem']))

            # ì½œë¡ ì´ ìˆëŠ” ì •ë³´ì„± ë¼ì¸ (ê°•ì¡° ì²˜ë¦¬)
            elif ':' in line and not line.startswith('**'):
                # ì´ì „ ë¬¸ë‹¨ ì™„ë£Œ
                if current_paragraph:
                    elements.append(Paragraph(' '.join(current_paragraph), self.styles['CustomBody']))
                    current_paragraph = []

                elements.append(Paragraph(line, self.styles['InfoText']))

            # ì¼ë°˜ í…ìŠ¤íŠ¸
            else:
                current_paragraph.append(line)

        # ë§ˆì§€ë§‰ ë¬¸ë‹¨ ì²˜ë¦¬
        if current_paragraph:
            elements.append(Paragraph(' '.join(current_paragraph), self.styles['CustomBody']))

        return elements

    def generate_pdf_from_sections(self, sections_dict, output_path, title="ê²½ë¶ëŒ€í•™êµ ì…ì‹œìš”ê°• ìš”ì•½"):
        """
        ì„¹ì…˜ ë”•ì…”ë„ˆë¦¬ì—ì„œ ì§ì ‘ PDF íŒŒì¼ ìƒì„± (í‘œ ìƒì„± ì—†ìŒ)

        Args:
            sections_dict (dict): ì„¹ì…˜ëª…ì„ í‚¤ë¡œ, ë‚´ìš©ì„ ê°’ìœ¼ë¡œ í•˜ëŠ” ë”•ì…”ë„ˆë¦¬
            output_path (str): ì¶œë ¥ PDF íŒŒì¼ ê²½ë¡œ
            title (str): PDF ì œëª©
        """
        doc = SimpleDocTemplate(
            output_path,
            pagesize=A4,
            rightMargin=20 * mm,
            leftMargin=20 * mm,
            topMargin=25 * mm,
            bottomMargin=25 * mm
        )

        # ìŠ¤í† ë¦¬ ìš”ì†Œë“¤
        story = []

        # ì œëª©
        story.append(Paragraph(title, self.styles['CustomTitle']))
        story.append(Spacer(1, 20))

        # ì„¹ì…˜ë³„ ì²˜ë¦¬
        for section_title, section_content in sections_dict.items():
            if not section_content or not str(section_content).strip():
                continue

            # ì„¹ì…˜ í—¤ë”
            story.append(Paragraph(section_title, self.styles['SectionHeader']))

            # ì„¹ì…˜ ë‚´ìš© í¬ë§·íŒ… ë° ì¶”ê°€
            elements = self.format_section_content(section_content)
            story.extend(elements)

            # ì„¹ì…˜ ê°„ ì—¬ë°±
            story.append(Spacer(1, 15))

        # PDF ìƒì„±
        try:
            doc.build(story)
            print(f"PDFê°€ ì„±ê³µì ìœ¼ë¡œ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤: {output_path}")
            return True
        except Exception as e:
            print(f"PDF ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return False

def extract_title_from_pdf_path(pdf_path):
    # íŒŒì¼ëª…ë§Œ ì¶”ì¶œ (í™•ì¥ì ì œê±°)
    filename = os.path.splitext(os.path.basename(pdf_path))[0]

    # '(í™ˆ)_' ì œê±°
    title = filename.replace('(í™ˆ)_', ' ')

    # ì—°ë„ íŒ¨í„´ ì°¾ê¸° (4ìë¦¬ ìˆ«ì)
    year_match = re.search(r'(\d{4})', title)
    if year_match:
        year = year_match.group(1)
        # ì—°ë„ë¥¼ 'ì—°ë„í•™ë…„ë„'ë¡œ ë³€ê²½
        title = title.replace(year, f'{year}í•™ë…„ë„')

    # ë§¨ ë’¤ì— 'ìš”ì•½' ì¶”ê°€
    title = title + ' ìš”ì•½'

    return title

# def process_pdfs(input_folder):
#     output_folder = input_folder / "summaries"
#     output_folder.mkdir(exist_ok=True)
#
#     pdfs = list(input_folder.glob('*.pdf'))
#     if not pdfs:
#         print("PDF íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
#         return
#
#     for pdf_path in pdfs:
#         print(f"\n{'=' * 60}")
#         print(f"ğŸ“„ {pdf_path.name} ì²˜ë¦¬ ì¤‘...")
#         print('=' * 60)
#
#         try:
#             summary =

# ì‚¬ìš© ì˜ˆì‹œ
# if __name__ == "__main__":
#
#     pdf_path = "C:\ì…ì‹œ-pdf-ìš”ì•½-í…ŒìŠ¤íŠ¸\ê²½ë¶ëŒ€(í™ˆ)_2025 ìˆ˜ì‹œ ëª¨ì§‘ìš”ê°•.pdf"
#     new_txt_path = pdf_path.replace(".pdf", "_(ìš”ì•½).txt")
#     new_pdf_path = pdf_path.replace(".pdf", "_(ìš”ì•½).pdf")
#     title = extract_title_from_pdf_path(pdf_path)
#
#     # ì´ˆê¸°í™”
#     summarizer = AdmissionSummarizer(openai_api_key=your_api_key)
#
#     # PDF ìš”ì•½ ìƒì„±
#     summary = summarizer.generate_full_summary(pdf_path)
#
#     # ìµœì¢… ìš”ì•½ì„œ ìƒì„±
#     summary_text = "\n\n".join([
#         f"**{name}:**\n{content}"
#         for name, content in summary.items()
#     ])
#
#     print("summary_text:")
#     print(summary_text)
#
#     # print("summary:")
#     # print(summary)
#
#     print("DEBUG TEXT:", DEBUG_TEXT)
#     test_evaluator(original=DEBUG_TEXT, summary=summary_text)
#
#     # ê²°ê³¼ ì €ì¥
#     with open(new_txt_path, "w", encoding="utf-8") as f:
#         f.write(summary_text)
#
#     # PDF ìƒì„±ê¸° ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
#     generator = SimpleAdmissionPDFGenerator()
#
#     # PDF ìƒì„±
#     generator.generate_pdf_from_sections(
#         sections_dict=summary,
#         output_path=new_pdf_path,
#         title=title
#     )
#
#     print("ì…ì‹œ ìš”ê°• ìš”ì•½ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")

import os
from pathlib import Path
import traceback


def process_pdf_folder(input_folder, your_api_key):
    """í´ë” ë‚´ ëª¨ë“  PDF íŒŒì¼ì„ ì²˜ë¦¬í•˜ëŠ” í•¨ìˆ˜"""
    input_folder = Path(input_folder)

    if not input_folder.exists():
        print(f"âŒ í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {input_folder}")
        return

    # PDF íŒŒì¼ ì°¾ê¸°
    pdf_files = list(input_folder.glob("*.pdf"))
    if not pdf_files:
        print(f"âŒ {input_folder}ì—ì„œ PDF íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return

    print(f"ğŸ“ ë°œê²¬ëœ PDF íŒŒì¼: {len(pdf_files)}ê°œ")
    for pdf_file in pdf_files:
        print(f"  - {pdf_file.name}")

    # ì¶œë ¥ í´ë” ì„¤ì •
    output_folder = input_folder / "summaries"
    output_folder.mkdir(exist_ok=True)
    print(f"ğŸ“‚ ìš”ì•½ íŒŒì¼ë“¤ì€ '{output_folder}' í´ë”ì— ì €ì¥ë©ë‹ˆë‹¤.")

    # ì²˜ë¦¬ ê²°ê³¼ ì¶”ì 
    success_count = 0
    failure_count = 0
    results = []

    # ê° PDF íŒŒì¼ ì²˜ë¦¬
    for i, pdf_path in enumerate(pdf_files, 1):
        print(f"\n{'=' * 60}")
        print(f"ğŸ“„ [{i}/{len(pdf_files)}] {pdf_path.name} ì²˜ë¦¬ ì¤‘...")
        print('=' * 60)

        try:
            # ì¶œë ¥ íŒŒì¼ ê²½ë¡œ ì„¤ì •
            new_txt_path = output_folder / f"{pdf_path.stem}_(ìš”ì•½).txt"
            new_pdf_path = output_folder / f"{pdf_path.stem}_(ìš”ì•½).pdf"

            title = extract_title_from_pdf_path(str(pdf_path))

            # ì´ˆê¸°í™”
            summarizer = AdmissionSummarizer(openai_api_key=your_api_key)

            # PDF ìš”ì•½ ìƒì„±
            print(f"ğŸ“„ {pdf_path.name} ìš”ì•½ ìƒì„± ì¤‘...")
            summary = summarizer.generate_full_summary(str(pdf_path))

            # ìµœì¢… ìš”ì•½ì„œ ìƒì„±
            summary_text = "\n\n".join([
                f"**{name}:**\n{content}"
                for name, content in summary.items()
            ])

            print(f"ğŸ“ ìš”ì•½ í…ìŠ¤íŠ¸ ê¸¸ì´: {len(summary_text)} ë¬¸ì")

            # ë””ë²„ê·¸ ë° í‰ê°€ (í•„ìš”ì‹œ)
            if 'DEBUG_TEXT' in globals():
                print("DEBUG TEXT:", DEBUG_TEXT)
                test_evaluator(original=DEBUG_TEXT, summary=summary_text)

            # TXT ê²°ê³¼ ì €ì¥
            with open(new_txt_path, "w", encoding="utf-8") as f:
                f.write(summary_text)
            print(f"âœ… TXT ì €ì¥ ì™„ë£Œ: {new_txt_path}")

            # PDF ìƒì„±ê¸° ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
            generator = SimpleAdmissionPDFGenerator()

            # PDF ìƒì„±
            generator.generate_pdf_from_sections(
                sections_dict=summary,
                output_path=str(new_pdf_path),
                title=title
            )
            print(f"âœ… PDF ì €ì¥ ì™„ë£Œ: {new_pdf_path}")

            success_count += 1
            results.append((pdf_path.name, True, f"{pdf_path.name} ì²˜ë¦¬ ì™„ë£Œ"))

        except Exception as e:
            error_msg = f"{pdf_path.name} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}"
            print(f"âŒ {error_msg}")
            print(f"ìƒì„¸ ì˜¤ë¥˜:\n{traceback.format_exc()}")
            failure_count += 1
            results.append((pdf_path.name, False, error_msg))

    # ìµœì¢… ê²°ê³¼ ì¶œë ¥
    print(f"\n{'=' * 60}")
    print("ğŸ‰ ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ!")
    print('=' * 60)
    print(f"âœ… ì„±ê³µ: {success_count}ê°œ")
    print(f"âŒ ì‹¤íŒ¨: {failure_count}ê°œ")
    print(f"ğŸ“Š ì´ ì²˜ë¦¬: {len(pdf_files)}ê°œ")

    if failure_count > 0:
        print(f"\nâŒ ì‹¤íŒ¨í•œ íŒŒì¼ë“¤:")
        for filename, success, message in results:
            if not success:
                print(f"  - {filename}: {message}")


if __name__ == "__main__":
    # API í‚¤ ì„¤ì •
    your_api_key = "your_api_key"  # ì‹¤ì œ API í‚¤ë¡œ ë³€ê²½í•˜ì„¸ìš”

    # ì²˜ë¦¬í•  í´ë” ê²½ë¡œ
    input_folder = r"C:\ì…ì‹œ-pdf-ìš”ì•½-í…ŒìŠ¤íŠ¸"

    # í´ë” ë‚´ ëª¨ë“  PDF íŒŒì¼ ë°°ì¹˜ ì²˜ë¦¬
    process_pdf_folder(input_folder, your_api_key)

    print("\nğŸ‰ ëª¨ë“  ì‘ì—… ì™„ë£Œ!")
