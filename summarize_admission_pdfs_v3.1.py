from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain.schema import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import FAISS
from pathlib import Path
import pdfplumber
import tiktoken
import re
from tiktoken import encoding_for_model
import os


def count_tokens(text):
    encoder = encoding_for_model("gpt-4o")
    return len(encoder.encode(text))


def extract_text_and_tables(file_path):
    text = ""
    with pdfplumber.open(file_path) as pdf:
        for page in pdf.pages:
            # í…ìŠ¤íŠ¸ ì¶”ì¶œ (ë ˆì´ì•„ì›ƒ ìœ ì§€)
            page_text = page.extract_text(layout=True) or ""
            lines = page_text.splitlines()

            # ì…ì‹œìš”ê°• ê´€ë ¨ í‚¤ì›Œë“œ íŒ¨í„´ í™•ì¥
            patterns = [
                r"ì •ì›|ì¸ì›|ëª¨ì§‘", r"ì¼ì •|ì¼ì •í‘œ|ì‹œê°„í‘œ", r"ì ‘ìˆ˜|ì‹ ì²­|ì§€ì›",
                r"í•™ê³¼|ì „ê³µ|ê³„ì—´", r"í•©ê²©|ë°œí‘œ|ì„ ë°œ|ê²°ê³¼", r"ë©´ì ‘|ì¸í„°ë·°|êµ¬ìˆ |ì‹¤ê¸°",
                r"ì„œë¥˜|ì œì¶œ|ì¤€ë¹„ë¬¼", r"ì‹œí—˜|í‰ê°€|ê³ ì‚¬", r"ë“±ë¡ê¸ˆ|í•™ë¹„|ìˆ˜ì—…ë£Œ",
                r"ì¥í•™ê¸ˆ|ì§€ì›ê¸ˆ|í˜œíƒ", r"ì „í˜•|ë°©ë²•|ê¸°ì¤€", r"ìˆ˜ì‹œ|ì •ì‹œ|íŠ¹ë³„",
                r"ì›|ì²œì›|ë§Œì›", r"í•™ì |GPA|ì„±ì ", r"TOEIC|TOEFL|ì–´í•™",
                r"ê¸°ìˆ™ì‚¬|ìƒí™œê´€|ì£¼ê±°", r"êµí™˜|ì—°ìˆ˜|í•´ì™¸"
            ]

            # ê´€ë ¨ ë¼ì¸ í•„í„°ë§ (ë” í¬ê´„ì ìœ¼ë¡œ)
            relevant_lines = []
            for line in lines:
                line_stripped = line.strip()
                if not line_stripped or "í˜ì´ì§€" in line or "ê´‘ê³ " in line:
                    continue

                # í‚¤ì›Œë“œ ë§¤ì¹­ ë˜ëŠ” ë‚ ì§œ/ìˆ«ì íŒ¨í„´ ë§¤ì¹­
                if (any(re.search(p, line, re.IGNORECASE) for p in patterns) or
                        any(str(y) in line for y in range(2020, 2026)) or
                        any(month in line for month in
                            ["1ì›”", "2ì›”", "3ì›”", "4ì›”", "5ì›”", "6ì›”", "7ì›”", "8ì›”", "9ì›”", "10ì›”", "11ì›”", "12ì›”"]) or
                        any(day in line for day in [f"{i}ì¼" for i in range(1, 32)]) or
                        re.search(r'\d+ì›|\d+ëª…|\d+ì ', line) or  # ê¸ˆì•¡, ì¸ì›, ì ìˆ˜
                        re.search(r'\d{4}\.\d{1,2}\.\d{1,2}', line)):  # ë‚ ì§œ í˜•ì‹
                    relevant_lines.append(line)

            text += "\n".join(relevant_lines) + "\n"

            # í‘œ ì¶”ì¶œ
            tables = page.extract_tables()
            for table in tables:
                if table:  # ë¹ˆ í…Œì´ë¸” ì œì™¸
                    for row in table:
                        if row and any(cell for cell in row if cell):  # ë¹ˆ í–‰ ì œì™¸
                            text += " | ".join([str(cell) if cell else "" for cell in row]) + "\n"

    return [Document(page_content=text)]


def process_documents(file_path):
    # ë‹¨ê³„ 1: ë¬¸ì„œ ë¡œë“œ
    docs = extract_text_and_tables(file_path)
    print(f"ì¶”ì¶œëœ ë¬¸ì„œ ìˆ˜: {len(docs)}")

    # ì¶”ì¶œëœ í…ìŠ¤íŠ¸ ì €ì¥ (ë””ë²„ê¹…ìš©)
    debug_file = Path(file_path).parent / f"{Path(file_path).stem}_extracted.txt"
    with open(debug_file, "w", encoding="utf-8") as f:
        f.write(docs[0].page_content)

    print(f"ì¶”ì¶œëœ í…ìŠ¤íŠ¸ ë¯¸ë¦¬ë³´ê¸°:\n{docs[0].page_content[:1500]}...")
    print(f"ì¶”ì¶œëœ í…ìŠ¤íŠ¸ í† í° ìˆ˜: {count_tokens(docs[0].page_content)}")

    # ë‹¨ê³„ 2: ë¬¸ì„œ ë¶„í•  (ì²­í¬ í¬ê¸° ì¡°ì •)
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=2500,  # ì²­í¬ í¬ê¸° ì¤„ì„
        chunk_overlap=300,
        separators=["\n\n", "\n", ".", "!", "?", ",", " ", ""]
    )
    split_documents = text_splitter.split_documents(docs)
    print(f"ë¶„í• ëœ ë¬¸ì„œ ì¡°ê° ìˆ˜: {len(split_documents)}")

    # ë‹¨ê³„ 3: ì„ë² ë”© ìƒì„±
    embeddings = OpenAIEmbeddings()

    # ë‹¨ê³„ 4: ë²¡í„°ìŠ¤í† ì–´ ìƒì„±
    vectorstore = FAISS.from_documents(documents=split_documents, embedding=embeddings)

    # ë‹¨ê³„ 5: ê²€ìƒ‰ê¸° ìƒì„± (ë” ë§ì€ ì¡°ê° ê²€ìƒ‰)
    retriever = vectorstore.as_retriever(
        search_type="mmr",
        search_kwargs={"k": 20, "fetch_k": 30}  # ë” ë§ì€ ì¡°ê° ê²€ìƒ‰
    )

    # ë‹¨ê³„ 6: ê°œì„ ëœ í”„ë¡¬í”„íŠ¸ ìƒì„±
    prompt = ChatPromptTemplate.from_messages([
        ("system", """ë‹¹ì‹ ì€ ëŒ€í•™ ì…ì‹œ ì „ë¬¸ ìƒë‹´ì‚¬ì…ë‹ˆë‹¤. 
ì…ì‹œìš”ê°• ë¬¸ì„œë¥¼ ì •í™•í•˜ê³  ì²´ê³„ì ìœ¼ë¡œ ë¶„ì„í•˜ì—¬ ìˆ˜í—˜ìƒê³¼ í•™ë¶€ëª¨ê°€ í•„ìš”ë¡œ í•˜ëŠ” í•µì‹¬ ì •ë³´ë¥¼ ëª…í™•í•˜ê²Œ ì •ë¦¬í•´ì£¼ì„¸ìš”.
ì •ë³´ê°€ ì—†ëŠ” ê²½ìš° "ì •ë³´ ì—†ìŒ" ë˜ëŠ” "ë¬¸ì„œì—ì„œ í™•ì¸ë˜ì§€ ì•ŠìŒ"ìœ¼ë¡œ í‘œì‹œí•˜ì„¸ìš”."""),

        ("human", """
ë‹¤ìŒì€ ëŒ€í•™ ì…ì‹œìš”ê°• PDFì—ì„œ ì¶”ì¶œí•œ í…ìŠ¤íŠ¸ì…ë‹ˆë‹¤. 
ì•„ë˜ í˜•ì‹ì— ë§ì¶° ì •í™•í•˜ê²Œ ìš”ì•½í•´ì£¼ì„¸ìš”:

## ğŸ“‹ ì…ì‹œ ì¼ì •
ê° ì „í˜•ë³„ ì£¼ìš” ì¼ì •ì„ ì‹œê°„ìˆœìœ¼ë¡œ ì •ë¦¬í•´ì£¼ì„¸ìš”.
í˜•ì‹ ì˜ˆì‹œ:
1. **ì›ì„œì ‘ìˆ˜**:
   * ê¸°ê°„: 2024ë…„ 9ì›” 9ì¼(ì›”) 09:00 ~ 9ì›” 13ì¼(ê¸ˆ) 18:00
   * ë°©ë²•: ì˜¨ë¼ì¸ ì ‘ìˆ˜

2. **ì„œë¥˜ì œì¶œ**:
   * ê¸°ê°„: 2024ë…„ 9ì›” 9ì¼(ì›”) ~ 9ì›” 20ì¼(ê¸ˆ)
   * ì œì¶œì²˜: ì…í•™ì²˜ ë˜ëŠ” ì˜¨ë¼ì¸

3. **1ë‹¨ê³„ í•©ê²©ì ë°œí‘œ**:
   * ì‹¤ê¸°/ì‹¤ì  ì „í˜•: 2024ë…„ 10ì›” 10ì¼(ëª©) 16:00
   * í•™ìƒë¶€ì¢…í•©ì „í˜•: 2024ë…„ 11ì›” 1ì¼(ê¸ˆ) 16:00

---

## ğŸ‘¥ ëª¨ì§‘ ì •ì›
í•™ê³¼ë³„ ë˜ëŠ” ê³„ì—´ë³„ ëª¨ì§‘ì¸ì›ì„ ì •ë¦¬í•´ì£¼ì„¸ìš”.
í˜•ì‹ ì˜ˆì‹œ:
* **ì˜ê³¼ëŒ€í•™**: 50ëª…
* **ê³µê³¼ëŒ€í•™**: 200ëª…
* **ì¸ë¬¸ëŒ€í•™**: 150ëª…

---

## ğŸ’° ë“±ë¡ê¸ˆ
í•™ê³¼ë³„ ë˜ëŠ” ê³„ì—´ë³„ ë“±ë¡ê¸ˆì„ ì •ë¦¬í•´ì£¼ì„¸ìš”.
í˜•ì‹ ì˜ˆì‹œ:
* **ì˜ê³¼ëŒ€í•™(ì˜ì˜ˆê³¼)**: 3,156,500ì›
* **ì¹˜ê³¼ëŒ€í•™(ì¹˜ì˜ì˜ˆê³¼)**: 3,156,500ì›
* **ê³µê³¼ëŒ€í•™/ITëŒ€í•™**: 2,331,000ì›
* **ì¸ë¬¸ëŒ€í•™**: 1,781,000ì›

---

## ğŸ“š ì…ì‹œ ì „í˜•
ì „í˜• ìœ í˜•ë³„ í‰ê°€ ë°©ë²•ê³¼ ë¹„ìœ¨ì„ ì •ë¦¬í•´ì£¼ì„¸ìš”.
í˜•ì‹ ì˜ˆì‹œ:
### ìˆ˜ì‹œëª¨ì§‘
* **í•™ìƒë¶€ì¢…í•©ì „í˜•**: ì„œë¥˜í‰ê°€ 70% + ë©´ì ‘ 30%
* **í•™ìƒë¶€êµê³¼ì „í˜•**: í•™ìƒë¶€ 100%

### ì •ì‹œëª¨ì§‘
* **ê°€êµ°**: ìˆ˜ëŠ¥ 100%
* **ë‚˜êµ°**: ìˆ˜ëŠ¥ 80% + í•™ìƒë¶€ 20%

---

## ğŸ“ ì¥í•™ê¸ˆ
ì¥í•™ê¸ˆ ì¢…ë¥˜ì™€ ì§€ì› ì¡°ê±´ì„ ì •ë¦¬í•´ì£¼ì„¸ìš”.
í˜•ì‹ ì˜ˆì‹œ:
* **ì„±ì ìš°ìˆ˜ì¥í•™ê¸ˆ**: ì…í•™ì„±ì  ìƒìœ„ 10% ì´ë‚´, ë“±ë¡ê¸ˆ ì „ì•¡
* **ì§€ì—­ì¸ì¬ì¥í•™ê¸ˆ**: íŠ¹ì • ì§€ì—­ ì¶œì‹ , ë“±ë¡ê¸ˆ 50%

---

## ğŸ“Œ ê¸°íƒ€ ì¤‘ìš”ì‚¬í•­
ê¸°ìˆ™ì‚¬, êµí™˜í•™ìƒ, íŠ¹ë³„ì „í˜• ë“± ê¸°íƒ€ ì •ë³´ë¥¼ ì •ë¦¬í•´ì£¼ì„¸ìš”.

**ì¤‘ìš”í•œ ì§€ì¹¨:**
1. ì •í™•í•œ ë‚ ì§œëŠ” ë°˜ë“œì‹œ "YYYYë…„ MMì›” DDì¼(ìš”ì¼)" í˜•ì‹ìœ¼ë¡œ ì‘ì„±
2. ê¸ˆì•¡ì€ ì •í™•í•œ ìˆ«ìë¡œ í‘œì‹œ (ì˜ˆ: 1,500,000ì›)
3. ì •ë³´ê°€ ì—†ìœ¼ë©´ "ì •ë³´ ì—†ìŒ" ë˜ëŠ” "ë¬¸ì„œì—ì„œ í™•ì¸ë˜ì§€ ì•ŠìŒ"ìœ¼ë¡œ í‘œì‹œ
4. ì¶”ì¸¡í•˜ì§€ ë§ê³  ë¬¸ì„œì— ëª…ì‹œëœ ë‚´ìš©ë§Œ ì‘ì„±
5. ì „í˜•ëª…, í•™ê³¼ëª…ì€ ë¬¸ì„œì˜ ì •í™•í•œ ëª…ì¹­ ì‚¬ìš©

**ë¶„ì„í•  í…ìŠ¤íŠ¸:**
{context}
""")
    ])

    # ë‹¨ê³„ 7: ì–¸ì–´ëª¨ë¸ ìƒì„± (temperature ì¡°ì •)
    llm = ChatOpenAI(model_name="gpt-4o", temperature=0.1)

    # ë‹¨ê³„ 8: ì²´ì¸ ìƒì„±
    def format_docs(docs):
        return "\n\n".join(doc.page_content for doc in docs)

    # ê²€ìƒ‰ ì¿¼ë¦¬ ê°œì„ 
    search_queries = [
        "ì…ì‹œ ì¼ì • ì›ì„œì ‘ìˆ˜ í•©ê²©ë°œí‘œ ë©´ì ‘ ê³ ì‚¬ì¼",
        "ëª¨ì§‘ì •ì› ëª¨ì§‘ì¸ì› í•™ê³¼ë³„ ì •ì›",
        "ë“±ë¡ê¸ˆ í•™ë¹„ ìˆ˜ì—…ë£Œ",
        "ì „í˜•ë°©ë²• í‰ê°€ê¸°ì¤€ ìˆ˜ì‹œ ì •ì‹œ",
        "ì¥í•™ê¸ˆ ì§€ì›ê¸ˆ í˜œíƒ",
        "ê¸°ìˆ™ì‚¬ êµí™˜í•™ìƒ íŠ¹ë³„ì „í˜•"
    ]

    # ëª¨ë“  ê²€ìƒ‰ ê²°ê³¼ í†µí•©
    all_docs = []
    for query in search_queries:
        docs = retriever.invoke(query)
        all_docs.extend(docs)

    # ì¤‘ë³µ ì œê±° (ë‚´ìš© ê¸°ì¤€)
    unique_docs = []
    seen_content = set()
    for doc in all_docs:
        if doc.page_content not in seen_content:
            unique_docs.append(doc)
            seen_content.add(doc.page_content)

    context = format_docs(unique_docs)

    print(f"ê²€ìƒ‰ëœ ë¬¸ì„œ ì¡°ê° ìˆ˜: {len(unique_docs)}")
    print(f"ì»¨í…ìŠ¤íŠ¸ í† í° ìˆ˜: {count_tokens(context)}")

    # ì²´ì¸ ì‹¤í–‰
    chain = prompt | llm | StrOutputParser()

    try:
        summary_res = chain.invoke({"context": context})
        print("\n" + "=" * 50)
        print("ğŸ“Š ì…ì‹œìš”ê°• ìš”ì•½ ê²°ê³¼")
        print("=" * 50)
        print(summary_res)
        print("=" * 50)

        return summary_res
    except Exception as e:
        print(f"ìš”ì•½ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None


def process_files(input_folder):
    input_folder = Path(input_folder)
    output_folder = input_folder / "summaries"
    output_folder.mkdir(exist_ok=True)

    pdf_files = list(input_folder.glob("*.pdf"))
    if not pdf_files:
        print("PDF íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return

    for file_path in pdf_files:
        print(f"\n{'=' * 60}")
        print(f"ğŸ“„ {file_path.name} ì²˜ë¦¬ ì¤‘...")
        print('=' * 60)

        try:
            summary = process_documents(file_path)
            if summary:
                # ìš”ì•½ ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥
                output_file = output_folder / f"{file_path.stem}_ìš”ì•½.txt"
                with open(output_file, "w", encoding="utf-8") as f:
                    f.write(f"ğŸ« {file_path.name} ì…ì‹œìš”ê°• ìš”ì•½\n")
                    f.write("=" * 60 + "\n\n")
                    f.write(summary)
                print(f"âœ… ìš”ì•½ ì™„ë£Œ: {output_file}")
            else:
                print(f"âŒ {file_path.name} ìš”ì•½ ì‹¤íŒ¨")
        except Exception as e:
            print(f"âŒ {file_path.name} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")


if __name__ == "__main__":
    # í™˜ê²½ë³€ìˆ˜ì—ì„œ OpenAI API í‚¤ í™•ì¸
    if not os.getenv("OPENAI_API_KEY"):
        print("âš ï¸  OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")
        exit(1)

    input_folder = r"C:\ì…ì‹œ-pdf-ìš”ì•½-í…ŒìŠ¤íŠ¸"

    if not Path(input_folder).exists():
        print(f"âŒ í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {input_folder}")
        exit(1)

    process_files(input_folder)
    print("\nğŸ‰ ëª¨ë“  ì‘ì—… ì™„ë£Œ!")